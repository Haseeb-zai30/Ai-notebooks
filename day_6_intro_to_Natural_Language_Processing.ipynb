{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPMtVDSa/uhvXZFU5KAVdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haseeb-zai30/Ai-notebooks/blob/main/day_6_intro_to_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to NLP\n",
        "NLP = Teaching computers to understand and use human language.\n",
        "\n",
        "NLP is the science of enabling computers to read, understand, and generate human language. It transforms raw text or speech into structured data so that machines can perform meaningful tasks.\n",
        "\n",
        "Examples of NLP tasks\n",
        "\n",
        "### Chatbots\n",
        "\n",
        "Input: \"What’s my next meeting?\"\n",
        "\n",
        "Output: \"Your team sync is at 10 AM.\"\n",
        "\n",
        "Helps machines talk like humans.\n",
        "\n",
        "###Spell-check & Auto-correct\n",
        "\n",
        "Input: \"I can’t wrte this\"\n",
        "\n",
        "Output: \"Did you mean ‘write’?\"\n",
        "\n",
        " Fixes typing mistakes.\n",
        "\n",
        "###Sentiment Analysis\n",
        "\n",
        "Input: \"I absolutely loved to watch football.\"\n",
        "\n",
        "Output: Positive\n",
        "\n",
        "Finds feelings (positive, negative, neutral) in text"
      ],
      "metadata": {
        "id": "LlzKrQizxe7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (install required packages)"
      ],
      "metadata": {
        "id": "CzLphmCvxm5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sIy9-7iQwNzl"
      },
      "outputs": [],
      "source": [
        "# Run this cell first in Colab to install packages that may not be preinstalled.\n",
        "# We keep installs minimal and common: nltk, spacy, scikit-learn, sentence-transformers (optional), and transformers.\n",
        "# If you are on Colab, these commands will download models the first time.\n",
        "\n",
        "\n",
        "!pip install --quiet nltk spacy scikit-learn sentence-transformers transformers textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Workflow Steps\n",
        "\n",
        "These steps clean the text so machine learning models can focus on important words instead of noise.\n",
        "\n",
        "### Lexical Analysis\n",
        "\n",
        "This step processes raw text into tokens and standardized forms. Operations include:\n",
        "\n",
        "**1. Tokenization**\n",
        "\n",
        "Breaking text into smaller pieces (words or sentences).\n",
        "\n",
        "Example:\n",
        "\"I love NLP.\"→ [\"I\", \"love\", \"NLP\", \".\"]\n",
        "\n",
        "**2. Stopword Removal**\n",
        "\n",
        "Removing common words that don’t add much meaning.\n",
        "\n",
        "Example:\n",
        "\"I love NLP\" → after removing \"I\" → [\"love\", \"NLP\"]\n",
        "\n",
        "\n",
        "**3. Stemming & Lemmatization**\n",
        "\n",
        "Stemming: Cuts words to their root form (may not be a real word).\n",
        "Example: \"playing\" → \"play\" , \"studies\" → \"studi\"\n",
        "\n",
        "Lemmatization: Converts word to its base dictionary form.\n",
        "Example: \"playing\" → \"play\" , \"studies\" → \"study\"\n",
        "\n",
        "\n",
        "**4. Normalization**\n",
        "\n",
        "Making text clean and consistent.\n",
        "\n",
        "Includes:\n",
        "\n",
        "Lowercasing: \"NLP\" → \"nlp\"\n",
        "\n",
        "Removing punctuation: \"Hello!!!\" → \"Hello\"\n",
        "\n",
        "Removing URLs: \"Check http://example.com\" → \"Check\""
      ],
      "metadata": {
        "id": "-BcBMdoRzTdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk = Natural Language Toolkit, a popular NLP library\n",
        "import nltk\n",
        "\n",
        "# re = Regular expressions (for pattern matching, e.g. removing URLs or special text)\n",
        "import re\n",
        "\n",
        "# unicodedata = Helps handle different Unicode characters (e.g., accented letters, emojis)\n",
        "import unicodedata\n",
        "\n",
        "# string = Python’s built-in string functions (e.g., punctuation removal)\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"punkt\")       # for tokenization\n",
        "nltk.download(\"punkt_tab\")   # new requirement in NLTK 3.9+\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Dde_ruygga",
        "outputId": "9395527a-14bf-4d8d-d8f1-22fc3766605c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_tokenize = splits sentences into words (Tokenization)\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# stopwords = list of common words (like \"the\", \"is\", \"and\") that we usually remove\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# PorterStemmer = algorithm for stemming (reducing words to root form, e.g., \"running\" → \"run\")\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# WordNetLemmatizer = tool for lemmatization (reduces words to their dictionary form, e.g., \"better\" → \"good\")\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "y7fEVn5n0fk2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'punkt' = tokenizer models (used by word_tokenize to split sentences/words)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# 'stopwords' = list of common words (like \"the\", \"is\", \"and\") for stopword removal\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# 'wordnet' = lexical database for English (used by WordNetLemmatizer for lemmatization)\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jb0V00q1P0b",
        "outputId": "15bf3689-144a-4991-df1e-05265ff558f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"I loved the new hotel downtown! The rooms were amazing . Check out their website: https://hoteldowntown.com\""
      ],
      "metadata": {
        "id": "wjo4NfGw1XsQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    # 1. Convert all text to lowercase (consistency)\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove accents / special characters (e.g., café → cafe)\n",
        "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "    # 3. Remove URLs (anything starting with http or www)\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "\n",
        "    # 4. Remove mentions (e.g., @username in social media)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "\n",
        "    # 5. Remove hashtags (e.g., #NLP → removed)\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)\n",
        "\n",
        "    # 6. Remove punctuation (replace with space to avoid word merging)\n",
        "    text = re.sub(r\"[{}]\".format(re.escape(string.punctuation)), \" \", text)\n",
        "\n",
        "    # 7. Remove extra spaces (convert multiple spaces into one)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "ocDq_ZtK2a8J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization:\n",
        "Input:\n",
        "\n",
        "norm = \"i loved the new hotel downtown the rooms were amazing check out their website\"\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "Tokens: ['i', 'loved', 'the', 'new', 'hotel', 'downtown', 'the', 'rooms', 'were', 'amazing', 'check', 'out', 'their', 'website']\n"
      ],
      "metadata": {
        "id": "WCdEVxBB3tEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply normalization function to the sample text\n",
        "norm = normalize_text(sample)\n",
        "\n",
        "# Print the result after normalization\n",
        "print(\"Normalized:\", norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPBo9SuA2cKu",
        "outputId": "f93ea6cd-ec7b-4886-aae9-5f058b3b2e8d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized: i loved the new hotel downtown the rooms were amazing check out their website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the normalized text into individual words (tokens)\n",
        "tokens = word_tokenize(norm)\n",
        "\n",
        "# Print the list of tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h07QDEd2t-0",
        "outputId": "a6b7c1fb-f4e3-4c21-d4c3-ac75a989ed99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['i', 'loved', 'the', 'new', 'hotel', 'downtown', 'the', 'rooms', 'were', 'amazing', 'check', 'out', 'their', 'website']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Word Removal:"
      ],
      "metadata": {
        "id": "Oq8QCM2W3aFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input tokens:\n",
        "\n",
        "['i', 'loved', 'the', 'new', 'hotel', 'downtown', 'the', 'rooms', 'were', 'amazing', 'check', 'out', 'their', 'website']\n",
        "\n",
        "\n",
        "After stopword removal:\n",
        "\n",
        "['loved', 'new', 'hotel', 'downtown', 'rooms', 'amazing', 'check', 'website']"
      ],
      "metadata": {
        "id": "s90p0Prw37JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of English stopwords from NLTK (e.g., \"the\", \"is\", \"and\")\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "JMt0jpuu3AZr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords: keep only words not in stop_words\n",
        "filtered = [w for w in tokens if w not in stop_words]\n"
      ],
      "metadata": {
        "id": "sbhVrtfT3f92"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokens after removing stopwords\n",
        "print(\"Without Stopwords:\", filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9MS5F_F3iix",
        "outputId": "919a24a0-2998-4dcb-8892-bc070c6bb27b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Stopwords: ['loved', 'new', 'hotel', 'downtown', 'rooms', 'amazing', 'check', 'website']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming vs Lemmatization\n",
        "\n",
        "Stemming: Shortens words to their “root” form by chopping off endings\n",
        "Uses simple rules like removing -ing, -ed, -s.\n",
        "\n",
        "Example:\n",
        "\n",
        "\"running\" → \"run\"\n",
        "\n",
        "\"cats\" → \"cat\"\n",
        "\n",
        "\"amazing\" → \"amaz\" (not a real word)\n",
        "\n",
        "\n",
        "Lemmatization: Uses vocabulary/dictionary → produces valid words or\n",
        "Converts words to their dictionary/base form (called lemma)\n",
        "\n",
        "Looks at word meaning and part of speech (noun, verb, adjective, etc.) using a vocabulary/dictionary.\n",
        "\n",
        "Example:\n",
        "\n",
        "\"running\" → \"run\"\n",
        "\n",
        "\"cats\" → \"cat\"\n",
        "\n",
        "\"better\" → \"good\" (if POS is adjective)"
      ],
      "metadata": {
        "id": "qdRTxkvz4a10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "9A3YeAXN3lB6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming\n",
        "stems = [stemmer.stem(w) for w in tokens]"
      ],
      "metadata": {
        "id": "Ud8zwW7P4mMH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming:\n",
        "Stemming chops words roughly to their root:\n",
        "\n",
        "\"loved\" → \"love\"\n",
        "\n",
        "\"rooms\" → \"room\"\n",
        "\n",
        "\"amazing\" → \"amaz\" (not a real word)\n",
        "\n",
        "\"website\" → \"websit\""
      ],
      "metadata": {
        "id": "mUOceX1x6td0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming\n",
        "stems = [stemmer.stem(w) for w in tokens]\n"
      ],
      "metadata": {
        "id": "Km9RrSyI4p4a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization:\n",
        "Lemmatization uses vocabulary rules → valid dictionary words:\n",
        "\n",
        "\"rooms\" → \"room\"\n",
        "\n",
        "\"amazing\" stays \"amazing\"\n",
        "\n",
        "\"loved\" stays \"loved\""
      ],
      "metadata": {
        "id": "uDj0II8P5CrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in tokens]"
      ],
      "metadata": {
        "id": "ZaH_D-M65I2N"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show results\n",
        "print(\"Original Words:     \", tokens)\n",
        "print(\"After Stemming:     \", stems)\n",
        "print(\"After Lemmatization:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGA-nbeg4szk",
        "outputId": "7205e4e9-f2a3-4601-9697-7ab4e5e39a16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:      ['i', 'loved', 'the', 'new', 'hotel', 'downtown', 'the', 'rooms', 'were', 'amazing', 'check', 'out', 'their', 'website']\n",
            "After Stemming:      ['i', 'love', 'the', 'new', 'hotel', 'downtown', 'the', 'room', 'were', 'amaz', 'check', 'out', 'their', 'websit']\n",
            "After Lemmatization: ['i', 'loved', 'the', 'new', 'hotel', 'downtown', 'the', 'room', 'were', 'amazing', 'check', 'out', 'their', 'website']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-at82ax4v8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}